{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "4NPxbuoHjlw2",
        "outputId": "a94cc8fa-3326-4638-e5a0-3edcf99306d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.1.0\n",
            "Collecting qdrant-client\n",
            "  Downloading qdrant_client-1.13.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (1.71.0)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client)\n",
            "  Downloading grpcio_tools-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.0.2)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.11.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.3.0)\n",
            "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.11/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (75.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->qdrant-client) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->qdrant-client) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.8->qdrant-client) (0.4.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Downloading qdrant_client-1.13.3-py3-none-any.whl (306 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.7/306.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_tools-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: portalocker, grpcio-tools, qdrant-client\n",
            "Successfully installed grpcio-tools-1.71.0 portalocker-2.10.1 qdrant-client-1.13.3\n",
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Collecting google-genai\n",
            "  Downloading google_genai-1.11.0-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Downloading google_genai-1.11.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.7/159.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-genai\n",
            "  Attempting uninstall: google-genai\n",
            "    Found existing installation: google-genai 1.10.0\n",
            "    Uninstalling google-genai-1.10.0:\n",
            "      Successfully uninstalled google-genai-1.10.0\n",
            "Successfully installed google-genai-1.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "de054e7d3f004b80be9ba47d9f22e694"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.1.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.4.0\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.7 [186 kB]\n",
            "Fetched 186 kB in 0s (898 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126332 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.7_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.7) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.7) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.52)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (0.3.31)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (2.11.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (3.10.16)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (1.3.1)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.9/433.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install dotenv\n",
        "!pip install qdrant-client\n",
        "!pip install --upgrade google-genai\n",
        "!pip install pdf2image\n",
        "!pip install pypdf\n",
        "!pip install pillow\n",
        "!apt-get install poppler-utils\n",
        "! pip install -qU langchain-qdrant\n",
        "! pip install langchain-core\n",
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LpF42J4jB3F",
        "outputId": "9db2d689-afe9-436d-d8c4-cb66bd991340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import google.generativeai as genai\n",
        "from google.genai.types import HttpOptions\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "from qdrant_client import QdrantClient\n",
        "from langchain_core.documents import Document\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from uuid import uuid4\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import glob\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DSBCRAy6jdfp"
      },
      "outputs": [],
      "source": [
        "\n",
        "#### ENVIRONMENT VARIABLES######\n",
        "QDRANT_KEY='xxxx'\n",
        "GEMINI_KEY='xxxx'\n",
        "QDRANT_URL='https://02d13de5-fafe-4288-822a-5aaca32e31a9.europe-west3-0.gcp.cloud.qdrant.io'\n",
        "OPEN_AI_KEY='xxxxx'\n",
        "\n",
        "##RETRIEVER CONFIGURATIONS###\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "hf = OpenAIEmbeddings(\n",
        "    api_key=OPEN_AI_KEY,\n",
        "    model=\"text-embedding-3-small\"\n",
        ")\n",
        "# Configure client all operation involving the db should be done through this object\n",
        "qdrant_vector_store=QdrantVectorStore.from_existing_collection( api_key=QDRANT_KEY ,\n",
        "                                                               embedding=hf,url=QDRANT_URL,\n",
        "                                                                collection_name='xxxx',\n",
        "                                                                prefer_grpc=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EW7isz89nlf3"
      },
      "outputs": [],
      "source": [
        "# Configure LLM model for pre-processing tasks\n",
        "genai.configure(api_key=GEMINI_KEY)\n",
        "model=genai.GenerativeModel('gemini-2.0-flash-001')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1mdAj8ejmEjh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d619b2e2-3b2c-4c88-b0b9-af94f933c793"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-11-7fbeb30ef692>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-7fbeb30ef692>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    '''''\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "###Logica de programa###\n",
        "'''\n",
        "Para convertir los archivos pdf a chunks jerarquicos se realizo el siguiente procedimiento:\n",
        "- convertimos individualmente cada unas de las paginas del archivo pdf a imagen  usando el metodo convert_to_images el resultado final se almacena en una carpeta por ejemplo 'PDF_IMAGES'\n",
        "- sobre las imagenes almacenadas usamos el LLM para extraer el contenido y generar archivos markdown usando el metodo ocr_with_gemini los archivos generados son almacenados en otra carpeta por ejemplo 'MARKDOWN_FILES'\n",
        "- los chunks son generados usando el mismo LLM usando la funcion create_chunks\n",
        "- para subir los chunks creados es necesario utilizar la funcion create_documents y upload_to_qdrant la primera funcion crea las abstracciones necesarias para realizar busquedas usando el framework langchain\n",
        "   la segunda funcion carga los datos usando el cliente de qdrant.\n",
        "'''\n",
        "\n",
        "## Procesamiento de imagenes\n",
        "def convert_to_images(pdf_path):\n",
        "  input_dir='/content/drive/MyDrive/Riesgo Financiero/IAS/'\n",
        "  output_dir='/content/drive/MyDrive/PDF_IMAGES/'\n",
        "  inppdf_path=input_dir+pdf_path\n",
        "  images= convert_from_path(inppdf_path)\n",
        "  image_paths=[]\n",
        "  for i,image in enumerate(images):\n",
        "    image_path=output_dir+pdf_path.split('.')[0]+f'_page_{i+1}.jpg'\n",
        "    image.save(image_path,'JPEG')\n",
        "    image_paths.append(image_path)\n",
        "  return image_paths\n",
        "\n",
        "\n",
        "# Generacion de markdown\n",
        "def ocr_with_gemini(image_paths,model):\n",
        "  images=[(path,Image.open(path))for path in image_paths]\n",
        "  prompt=f\"\"\"\n",
        "\n",
        "Extract the contents of the provided iamge into a Markdown format.  Pay close attention to preserving the document's structure, including headings, lists, and paragraphs. For tables, implement the following rules:\n",
        "\n",
        "1. **Ideal Extraction:** If the table's data can be accurately extracted, represent it in a Markdown table format.\n",
        "\n",
        "2. **Sanitary Extraction (Data Extraction Failure):** If the table data cannot be reliably extracted, provide a brief summary of the table's content in a Markdown paragraph. Include the number of rows and columns and a general description of the table's subject matter.  For example:  \"This table (5 rows x 3 columns) appears to summarize key financial ratios for Q3 2023.\"\n",
        "\n",
        "3. **Image Handling:** If a table is represented as an image, describe it in a similar manner to rule 2, referencing the image if possible.\n",
        "\n",
        "Ensure that all extracted content maintains semantic meaning. Use Markdown formatting consistently.\n",
        "\n",
        "\"\"\"\n",
        "  summary=\"\"\n",
        "\n",
        "  for path,image in images:\n",
        "      try:\n",
        "        response=model.generate_content([prompt,image])\n",
        "        summary+=response.text+'####End####'\n",
        "      except:\n",
        "        print(f'Error procesando la imagen {path}')\n",
        "        continue\n",
        "  return summary\n",
        "\n",
        "#creacion de chunks\n",
        "def create_chunks(path_file,chunk_size):\n",
        "  #Retrieve and open file\n",
        "  with open(path_file,'r') as file:\n",
        "    markdown_content=file.read()\n",
        "\n",
        "  markdown_content=markdown_content.split('####End####')\n",
        "  markdown_content= [ page_content+str(page_number).replace('@','') for page_number,page_content in enumerate(markdown_content)]\n",
        "\n",
        "  prompt_chunk=f\"\"\"\n",
        "  Based on the content of a file create text chunks of max {chunk_size} words\n",
        "  The file strucutre is a markdown hence use the following guidelines to handdle different types of content:\n",
        "    Preserve hierarchy: Use the heading levels to determine the chunk boundaries. For example:\n",
        "        - A *bold title* or ## begins a new chunk.\n",
        "        - A ### heading begins a sub-sub-section and so on.\n",
        "    Chunking rule: Each chunk should include:\n",
        "    Its full heading path as metadata (e.g., Chapter 1 > Section 1.1 > Subsection A).\n",
        "    All relevant content under that heading until the next heading of the same or higher level.\n",
        "    Maximum size per chunk (optional): If any section exceeds {chunk_size} words or tokens, split it further by semantic or paragraph boundaries, and maintain the heading path in each sub-chunk.\n",
        "\n",
        "  only provide the chunk information in the ouput dont icnlude any other extra information regarding your reasoning use the following strucuture in the for each chunk dont use json in your answer :\n",
        "  <chunk_information>\n",
        "  <heading_path>:most recent bold title or heading identified </heading_path>\n",
        "  <content>:The chunk content</content>\n",
        "  <page_numer>: use the number provided as input for this value</page_numer>\n",
        "  <name_file>: name of the file</name_file>\n",
        "\n",
        "  </chunk_information>\n",
        "  \"\"\"\n",
        "  chunk_list=[]\n",
        "  for markdown_chunk in markdown_content:\n",
        "    try:\n",
        "      response=model.generate_content([prompt_chunk,markdown_chunk])\n",
        "      chunk_list.append(response.text)\n",
        "    except:\n",
        "      print(f'Error procesando el chunk {markdown_chunk}')\n",
        "      continue\n",
        "\n",
        "  return chunk_list\n",
        "\n",
        "### Subir chunks a Qdrant###\n",
        "def create_documents(text_blob):\n",
        "  import re\n",
        "  chunks=re.findall(r\"<chunk_information>(.*?)</chunk_information>\", text_blob,re.DOTALL)\n",
        "  documents=[]\n",
        "  for chunk in chunks:\n",
        "    try:\n",
        "      heading_path=re.search(r\"<heading_path>(.*?)</heading_path>\", chunk).group(1) if re.search(r\"<heading_path>(.*?)</heading_path>\", chunk) else 'invalid_heading'\n",
        "      page_number=re.search(r\"<page_numer>(.*?)</page_numer>\", chunk).group(1)if re.search(r\"<page_numer>(.*?)</page_numer>\", chunk) else '99999'\n",
        "      name_file=re.search(r\"<name_file>(.*?)</name_file>\", chunk).group(1) if re.search(r\"<name_file>(.*?)</name_file>\", chunk) else 'invalid_name_file'\n",
        "      content=re.search(r\"<content>(.*?)</content>\", chunk,re.DOTALL).group(1) if re.search(r\"<content>(.*?)</content>\", chunk,re.DOTALL) else 'invalid_content'\n",
        "      if heading_path!='invalid_heading' and page_number!='invalid_page_number' and name_file!='invalid_name_file' and content!='invalid_content':\n",
        "            documents.append(Document(page_content=content,\n",
        "                              metadata={'heading_path':heading_path,'page_number':page_number,'name_file':name_file})\n",
        "            )\n",
        "      if heading_path=='invalid_heading' or page_number=='invalid_page_number' or name_file=='invalid_name_file' or content=='invalid_content':\n",
        "        dict_debuging={'heading_path':heading_path,'page_number':page_number,'name_file':name_file,'content':len(content)}\n",
        "        print(f'Error procesando el chunk document built:{dict_debuging}')\n",
        "        continue\n",
        "    except Exception as e:\n",
        "        continue;\n",
        "\n",
        "\n",
        "  return documents\n",
        "def upload_to_qdrant(document_list,db_client):\n",
        "  uuids = [str(uuid4()) for _ in range(len(document_list))]\n",
        "  db_client.add_documents(\n",
        "      documents=document_list,\n",
        "      ids=uuids)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "for  blob_text in glob.glob('/content/drive/MyDrive/MARKDOWN_FILES/*.txt'):\n",
        "  try:\n",
        "    with open(blob_text,'r') as f:\n",
        "      blob_content=f.read()\n",
        "    documents=create_documents(blob_content)\n",
        "    json_documents=[document.json() for document in documents]\n",
        "    with open(f'{blob_text.replace(\".txt\",\".json\")}','w') as f:\n",
        "      json.dump(json_documents,f)\n",
        "    upload_to_qdrant(documents,qdrant_vector_store)\n",
        "  except Exception as e:\n",
        "    raise e\n",
        "    continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7IRoD0kciCyk",
        "outputId": "4716aed7-e54f-42c0-a0c3-2f68d318bd75"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-ef81947ccab8>:7: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  json_documents=[document.json() for document in documents]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error procesando el chunk document built:{'heading_path': 'NIIF 9 > Capítulo 5 Medición', 'page_number': '22', 'name_file': 'NIIF 9', 'content': 15}\n",
            "Error procesando el chunk document built:{'heading_path': 'invalid_heading', 'page_number': ': 4 ', 'name_file': ': IAS 36', 'content': 2243}\n",
            "Error procesando el chunk document built:{'heading_path': 'invalid_heading', 'page_number': ':20', 'name_file': ': Información a revelar', 'content': 2358}\n",
            "Error procesando el chunk document built:{'heading_path': 'invalid_heading', 'page_number': '20', 'name_file': 'NIC 32', 'content': 1946}\n",
            "Error procesando el chunk document built:{'heading_path': 'invalid_heading', 'page_number': '42', 'name_file': 'NIC 32', 'content': 1268}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mdMjBGDdytyx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}